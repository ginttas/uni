{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2022-09-13\n",
    "\n",
    "### 2. Intro to ML\n",
    "\n",
    "#### First half: Philosophy / Concepts\n",
    "* ML is used when we know how the solution looks, but we don't known a algorithm.\n",
    "* Also it's used when a lot of data is used\n",
    "* ML - Field of study that givers computers the ability yo learn, without being explicitly programed to do so\n",
    "\n",
    "##### History\n",
    "* Math - ancient\n",
    "* Philosophical logic - 1st millennium BC\n",
    "* Mathematical calculators - 17 th century\n",
    "* Mathematical logics - 20th century\n",
    "* Computers - 1946\n",
    "* Birth of AI field, golden years - 1956\n",
    "* \"AI winter\" - 1974 - 2010? (Technology hype cycle)\n",
    "* \"deep learning\" - 2000\n",
    "* ...\n",
    "\n",
    "* Frank Rosenblatt: Perceptron - 1957\n",
    "\n",
    "##### ...\n",
    "* Leaning system that improves itself\n",
    "* Statistics + Computer\n",
    "* Intelligence is the ability to adapt to changes\n",
    "\n",
    "#### Limitation of ML, compared to naturlal\n",
    "* Humans have a lot of context\n",
    "* Algorithms needs a lot of labeled data\n",
    "* Computational power might still be too week (Compared to our brain)\n",
    "* Our understanding of the brain is very limited\n",
    "* Embeddedness and embodiment\n",
    "* AI similar to humans should have similar environment/body? (Actively learn)\n",
    "* Is human intelgents ideal and should we try to replicate it?\n",
    "* Whats is intelligence?\n",
    "\n",
    "* Kaggle - Competitive ML solving application (good place to learn ML) (Have prizes)\n",
    "* 20% - data cleaning\n",
    "* 40% - Data exploration, feature generation\n",
    "* 30% - Modeling\n",
    "* 10% - Ensample model\n",
    "\n",
    "#### ML vs Data science\n",
    "* Data science - understand data\n",
    "* ML - uses this data\n",
    "\n",
    "#### ...\n",
    "* In ML data quality of the data is important!!!\n",
    "* \"Garbage in garbage out\"\n",
    "* Better data wins if compared to an algorithm\n",
    "\n",
    "#### Second half: More technical stuff\n",
    "* Supervised learning - Leaning by close example\n",
    "* Unsupervised - \"Making sense\" of data\n",
    "* Reinforcement - Instead of supervised and unsupervised (We can reward or punish based on solutions)\n",
    "* Competition - Co evolution (https://www.youtube.com/watch?v=62IheUGZQLU)\n",
    "\n",
    "##### Types of learning\n",
    "* Offline learning - We train and then use\n",
    "* Online - Model always learns (even when in use) (In online, model could forget what it learned before)\n",
    "\n",
    "##### Mathematical abstractions\n",
    "\n",
    "##### Good model\n",
    "Should be:\n",
    "* Generic\n",
    "* High expressive power (Number of trainable parameters)\n",
    "* ComputATIONALY EFFICIENT\n",
    "* Easy to train\n",
    "* ...\n",
    "\n",
    "##### Supervised ML tasks:\n",
    "\n",
    "\n",
    "##### Examople:\n",
    "\n",
    "##### ...\n",
    "The curse of dimensionality\n",
    "* Its practically impossible to calculate probability density of high dimensional functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "667a7d0ce390a48f63afb7aae6a889c6556871eaae12ddfa514ec638112817df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
